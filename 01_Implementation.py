"""Personal Challenge_Draft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-25-B3CO6yVCH9u2vgbhIjyyFeU3tJ3w
"""

# Working environment set up
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import seaborn as sns
from nltk.corpus import wordnet
import matplotlib.pyplot as plt
from matplotlib.legend_handler import HandlerLine2D
import numpy as np

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')


def load_data():
    '''
    This function will separately return the features and response variable for the input data
    '''
    data = pd.read_csv('data.csv')
    X = data['Lyric']
    y = data['Genre']
    return X, y


# Use pos_tag to get the type of the world and then map the tag to the format wordnet lemmatizer would accept.
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)


def transform_data():
    '''
    This function will transform the features and will reuturn the countvectorized features.
    Steps are:
    1. Remove punctuations
    2. Tokenize
    3. Lemmatization
    4. Remove stop words
    5. CountVectorize
    '''
    X, y = load_data()
    X = X.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))  # To remove the punctuations
    X_Tokenize = X.apply(lambda x: word_tokenize(x))  # To tokenize

    lemmatizer = WordNetLemmatizer()
    X_lemmatize = X_Tokenize.apply(lambda x: ' '.join([lemmatizer.lemmatize(w, pos='v') for w in x]))

    stop_words = set(stopwords.words('english'))
    stop_words_more = ('10', '100', '20', '2x', '3x', '4x', '50', 'im')  # Add more stop words
    stop_words = stop_words.add(x for x in stop_words_more)

    CountVect = CountVectorizer(stop_words=stop_words, min_df=300, lowercase=True, ngram_range=(1, 1))
    Transformmed_array = CountVect.fit_transform(X_lemmatize)
    X_vectorized = pd.DataFrame(Transformmed_array.toarray(), columns=CountVect.get_feature_names())
    return X_vectorized, y


def EDA_visualize(X, y, N):
    '''
    :para X: X is the features to be trained
    :para y: y is the Gnere classification to be trained
    :para N: nlargest frequencied words for each type of Genre
    :return: 1. Barplot to visulize the counts for each type of y 2. Return the n largest frequencies words for each type of y
    '''
    sns.catplot(x='Genre', kind='count', data=pd.DataFrame(y[:50000]))
    DF_Combine = pd.concat([X, y], axis=1)

    DF_nlargest = pd.DataFrame(np.ones((3, 1)), columns=['exm'], index=['Hip Hop', 'Pop', 'Rock'])  # Initilnize
    for value in DF_Combine.columns[:-1]:
        DF_nlargest[value] = pd.DataFrame(DF_Combine.groupby('Genre')[value].sum())

    print(DF_nlargest.apply(lambda s, n: s.nlargest(n).index, axis=1, n=N))


# X_temp, y_temp = transform_data()

def TuneParameter_visulize(X_train, y_train, X_hold, y_hold):
    '''
    It will return severl plots aims to tune paramters.
    parameters are:
    1. max_depth
    2. n_estimators
    3. max_features...

    Todo: plotting more parameters
    '''
    # Tune max_depth
    max_depths = np.linspace(10, 200, 15, endpoint=True)
    train_results = []
    validation_results = []
    for depth in max_depths:
        rf = RandomForestClassifier(max_depth=depth, n_jobs=-1)
        rf.fit(X_train, y_train)
    train_results.append(accuracy_score(y_train, rf.predict(X_train)))
    validation_results.append(accuracy_score(y_hold, rf.predict(X_hold)))

    line1 = plt.plot(max_depths, train_results, 'b', label='Train accuracy')
    line2 = plt.plot(max_depths, validation_results, 'r', label='Estimated accuracy')
    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
    plt.ylabel('accuracy score')
    plt.xlabel('Tree depth')
    plt.show()


def main():
    '''
    It will return:
    1. EDA visulization
    2. Visulize parameter tuning process
    3. Series include Expected accuracy
    4. Series include the predicted y_test
    '''
    # Load data
    X_input, y_input = transform_data()

    # Train, holdset, test split
    y_test = pd.DataFrame(y_input[-5000:], columns=['Genre'])
    y_train = pd.DataFrame(y_input[:50000], columns=['Genre'])

    X_train = pd.DataFrame(X_input.iloc[:50000, :], columns=X_input.columns)
    X_test = pd.DataFrame(X_input.iloc[-5000:, :], columns=X_input.columns)

    X_holdout_set = X_train.sample(5000, random_state=66)
    y_holdout_set = y_train.iloc[X_holdout_set.index, :]
    X_train_new = X_train.drop(X_holdout_set.index)
    y_train_new = y_train.drop(X_holdout_set.index)

    EDA_visualize(X_train, y_train, 10)  # For EDA purpose

    # Build classifier
    '''
    The RF model will be used. Few reasons below:
    1. An ensemble (bootstrap) approach might make stronger predictions, without causing serious overfitting
    2. Compared with distance methods, it needs less datapreprocessing (such as scaling data) 
    3. Non-parametric estimation 
  
    However, it may have an obvious drawback: 
    1. May set large max_features
    2. Should consider more deeper depth
    The drawbacks above will directly triggle the large training workload.
    '''
    # TuneParameter_visulize(X_train_new,y_train_new, X_holdout_set, y_holdout_set) # Tune parameters

    RF_Model = RandomForestClassifier(criterion='entropy', n_estimators=100, max_depth=56, max_features=666)
    RF_Model.fit(X_train_new, y_train_new)

    estimated_accuracy = accuracy_score(y_holdout_set, RF_Model.predict(X_holdout_set))
    pd.Series(estimated_accuracy).to_csv('ea.csv', index=False, header=False)

    # Predict testing set
    test_pred = RF_Model.predict(X_test)
    pd.Series(test_pred).to_csv('pred.csv', index=False, header=False)


if __name__ == '__main__':
    main()
